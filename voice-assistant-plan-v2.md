# Voice Assistant V2: Bidirectional OpenClaw Voice Interface

**For:** Ray (hal-System-Product-Name)  
**Date:** 2026-02-21  
**Goal:** Transform the local voice assistant into a bidirectional voice interface for OpenClaw

---

## Executive Summary

V2 evolves the standalone voice assistant into a **bidirectional voice bridge** for OpenClaw. Users speak naturally â†’ OpenClaw processes with full tool access â†’ responses spoken back. Critical innovation: intelligent filtering to ensure only **final, user-facing responses** reach TTS, not internal tool calls, thinking, or planning.

**Key Architectural Shifts:**
- âŒ Direct Ollama connection (bypasses OpenClaw tools)
- âœ… OpenClaw-native processing (full tool access, MEMORY.md context, skills)
- âœ… Response filtering pipeline (eliminates internal chatter)
- âœ… Session-aware conversation context

---

## Architecture Overview

### V2 System Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           BIDIRECTIONAL VOICE INTERFACE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Microphone â”‚â”€â”€â”€â”€â†’â”‚  STT Engine      â”‚â”€â”€â”€â”€â†’â”‚  OpenClaw Voice Bridge       â”‚   â”‚
â”‚  â”‚             â”‚     â”‚  (Whisper)       â”‚     â”‚  (WebSocket/HTTP Client)     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚                   â”‚
â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                          â”‚                                                       â”‚
â”‚                          â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                         OPENCLAW SESSION                                â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚ Tool Engine  â”‚  â”‚ Memory       â”‚  â”‚ Reasoning    â”‚  â”‚ Skills       â”‚  â”‚   â”‚
â”‚  â”‚  â”‚ (Web Search, â”‚  â”‚ (MEMORY.md,  â”‚  â”‚ (Chain of    â”‚  â”‚ (Custom      â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  Files, etc) â”‚  â”‚  Context)    â”‚  â”‚  Thought)    â”‚  â”‚  Modules)    â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚                           â”‚                                              â”‚   â”‚
â”‚  â”‚                           â–¼                                              â”‚   â”‚
â”‚  â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚   â”‚
â”‚  â”‚                   â”‚ LLM Core     â”‚                                       â”‚   â”‚
â”‚  â”‚                   â”‚ (Ollama)     â”‚                                       â”‚   â”‚
â”‚  â”‚                   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚   â”‚
â”‚  â”‚                          â”‚                                               â”‚   â”‚
â”‚  â”‚                          â–¼                                               â”‚   â”‚
â”‚  â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚   â”‚
â”‚  â”‚                   â”‚ Response     â”‚                                       â”‚   â”‚
â”‚  â”‚                   â”‚ Filter       â”‚â—„â”€â”€ [METADATA: final=true/false]      â”‚   â”‚
â”‚  â”‚                   â”‚ (Final only) â”‚                                       â”‚   â”‚
â”‚  â”‚                   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                             â”‚                                                    â”‚
â”‚                             â”‚ WebSocket/HTTP                                     â”‚
â”‚                             â”‚ (tagged final response)                             â”‚
â”‚                             â–¼                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                          VOICE OUTPUT PIPELINE                           â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚   â”‚
â”‚  â”‚  â”‚  TTS Engine  â”‚â”€â”€â”€â”€â†’â”‚  Audio       â”‚â”€â”€â”€â”€â†’â”‚  Speakers    â”‚            â”‚   â”‚
â”‚  â”‚  â”‚  (Piper)     â”‚     â”‚  Playback    â”‚     â”‚              â”‚            â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  USER    â”‚â”€â”€â”€â†’â”‚  AUDIO   â”‚â”€â”€â”€â†’â”‚   STT    â”‚â”€â”€â”€â†’â”‚ OPENCLAW â”‚â”€â”€â”€â†’â”‚   LLM    â”‚
â”‚  SPEAKS  â”‚    â”‚  CAPTURE â”‚    â”‚ (TEXT)   â”‚    â”‚  BRIDGE  â”‚    â”‚ PROCESS  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                                                                    â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  USER    â”‚â†â”€â”€â”€â”‚  SPEAKER â”‚â†â”€â”€â”€â”‚   TTS    â”‚â†â”€â”€â”€â”‚ RESPONSE â”‚â†â”€â”€â”€â”‚ FILTER   â”‚
â”‚  HEARS   â”‚    â”‚  AUDIO   â”‚    â”‚ (VOICE)  â”‚    â”‚  QUEUE   â”‚    â”‚ (FINAL)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## OpenClaw Integration Protocol

### Protocol Comparison

| Aspect | WebSocket | HTTP Streaming | Recommendation |
|--------|-----------|----------------|----------------|
| **Latency** | Lowest (<50ms) | Medium (100-200ms) | WebSocket for voice |
| **Complexity** | Higher | Lower | HTTP for v2 MVP, WS for v2.5 |
| **Session State** | Built-in | Requires polling | WebSocket preferred |
| **OpenClaw Support** | Native (sessions) | Via REST API | Match OpenClaw capability |
| **Error Recovery** | Auto-reconnect | Retry logic needed | WebSocket auto-reconnect |

### Phase 1: HTTP Streaming (MVP)

Use OpenClaw's existing REST endpoints with Server-Sent Events (SSE):

```python
# voice_bridge/client.py
import requests
import json

class OpenClawVoiceClient:
    def __init__(self, base_url="http://localhost:3000"):
        self.base_url = base_url
        self.session_id = None
    
    def send_voice_input(self, text: str) -> str:
        """Send transcribed voice to OpenClaw, receive final response only"""
        url = f"{self.base_url}/api/voice/chat"
        
        payload = {
            "message": text,
            "session_id": self.session_id,
            "voice_mode": True,  # Signal: filter for final responses
            "channel": "voice"
        }
        
        response = requests.post(url, json=payload, stream=True)
        
        # Parse SSE stream for final response only
        final_response = ""
        for line in response.iter_lines():
            if line:
                event = json.loads(line.decode('utf-8'))
                if event.get("type") == "final":
                    final_response = event.get("content")
                    break
                # Skip: "thinking", "tool_call", "tool_result", "progress"
        
        return final_response
```

### Phase 2: WebSocket (Production)

WebSocket enables true bidirectional streaming and interruption handling:

```javascript
// voice_bridge/ws_client.js
class OpenClawVoiceSocket {
  constructor() {
    this.ws = new WebSocket('ws://localhost:3000/voice');
    this.audioQueue = [];
    this.setupHandlers();
  }
  
  setupHandlers() {
    this.ws.onmessage = (event) => {
      const msg = JSON.parse(event.data);
      
      switch(msg.type) {
        case 'text_delta':
          // Accumulate text, don't TTS yet
          this.accumulateText(msg.content);
          break;
          
        case 'final':
          // Only now trigger TTS
          this.speak(msg.content);
          break;
          
        case 'interruption':
          // User started speaking, cancel current output
          this.cancelPlayback();
          break;
          
        case 'session_id':
          this.sessionId = msg.content;
          break;
      }
    };
  }
  
  sendTranscription(text) {
    this.ws.send(JSON.stringify({
      action: 'transcribe',
      text: text,
      session_id: this.sessionId
    }));
  }
}
```

### OpenClaw API Requirements

OpenClaw needs these endpoints for voice integration:

```yaml
# POST /api/voice/chat (HTTP)
Request:
  message: string           # Transcribed user speech
  session_id: string?      # For conversation continuity
  voice_mode: true         # Enable filtering
  
Response (SSE):
  - event: thinking        # Skip TTS
  - event: tool_call      # Skip TTS
  - event: tool_result     # Skip TTS
  - event: delta          # Skip TTS (intermediate)
  - event: final          # âœ“ TTS THIS

# WebSocket /voice (Bidirectional)
Client â†’ Server:
  - transcribe: {text, session_id}
  - interruption: {}      # User started speaking
  - continue: {}           # Acknowledge end of turn
  
Server â†’ Client:
  - session_id: string
  - text_delta: string     # Accumulate but don't speak
  - final: string          # âœ“ TTS THIS
  - interruption: {}       # Clear TTS queue
```

---

## Response Filtering Strategy

### The Problem

OpenClaw's typical response contains:
```
[tool_call: web_search("weather today")]
[tool_result: "72Â°F, sunny"]
<thinking>Drafting response about nice weather...</thinking>
It's a beautiful 72Â°F with sunny skies today!
```

**Goal:** Only TTS the last line.

### Filtering Approaches

| Approach | How It Works | Pros | Cons |
|----------|--------------|------|------|
| **A: Metadata Tags** | OpenClaw adds `{"final": true}` markers | Explicit, reliable | Requires OpenClaw changes |
| **B: Heuristic Parser** | Detect thinking blocks, tool calls | No OpenClaw changes needed | Fragile, may miss edge cases |
| **C: Streaming Filter** | Filter tokens in real-time | Low latency | Complex to implement |

**Recommendation:** Use A with B as fallback.

### Implementation: Metadata-Based Filtering

```python
# openclaw/plugins/voice_filter.py

class VoiceFilterPlugin:
    """
    OpenClaw plugin that marks final vs internal responses
    for voice interfaces.
    """
    
    def on_response_start(self, context):
        context.is_final = False
        context.buffer = []
    
    def on_thinking_start(self, context):
        context.is_thinking = True
        # Emit metadata but don't queue for TTS
        yield {"type": "thinking", "speakable": False}
    
    def on_thinking_end(self, context):
        context.is_thinking = False
    
    def on_tool_call(self, tool_name, params, context):
        # Tool calls are never spoken
        yield {
            "type": "tool_call",
            "tool": tool_name,
            "speakable": False
        }
    
    def on_tool_result(self, result, context):
        # Tool results might be displayed but not spoken directly
        yield {
            "type": "tool_result", 
            "speakable": False,
            "display": True
        }
    
    def on_response_chunk(self, text, context):
        context.buffer.append(text)
        # Intermediate chunks accumulate but aren't final
        yield {"type": "delta", "content": text, "speakable": False}
    
    def on_response_end(self, context):
        # Mark accumulated buffer as final
        final_text = "".join(context.buffer)
        yield {
            "type": "final",
            "content": final_text,
            "speakable": True  # âœ“ This goes to TTS
        }
```

### Fallback: Heuristic Filter

If OpenClaw changes are delayed:

```python
# voice_bridge/fallback_filter.py
import re

class HeuristicVoiceFilter:
    """
    Client-side filter when OpenClaw doesn't provide metadata.
    Detects and removes internal content.
    """
    
    # Patterns to exclude from TTS
    SKIP_PATTERNS = [
        r'\[tool_call:.*?\]',           # Tool invocations
        r'\[tool_result:.*?\]',         # Tool results
        r'<thinking>.*?</thinking>',      # Thinking blocks
        r'<reasoning>.*?</reasoning>',    # Reasoning blocks
        r'<plan>.*?</plan>',              # Planning blocks
        r'Let me (search|check|look up|find|verify).*?\.',  # Self-directed actions
        r'I need to (search|check|look).*?\.',              # Planning language
        r'^(Searching|Checking|Looking up|Querying)\b',    # Action prefixes
    ]
    
    def filter(self, text: str) -> str | None:
        """Returns filtered text or None if should be skipped entirely"""
        
        # Check if it's purely internal
        for pattern in self.SKIP_PATTERNS:
            if re.match(pattern, text, re.DOTALL | re.IGNORECASE):
                return None
        
        # Remove embedded internal content
        filtered = text
        for pattern in self.SKIP_PATTERNS:
            filtered = re.sub(pattern, '', filtered, flags=re.DOTALL | re.IGNORECASE)
        
        # Clean up artifacts
        filtered = re.sub(r'\n+', ' ', filtered).strip()
        filtered = re.sub(r'\s+', ' ', filtered)
        
        return filtered if filtered else None
```

---

## Session Management

### Requirements

Voice conversations need **persistent sessions** to maintain:
- Conversation history (last N turns)
- OpenClaw context (loaded files, skills state)
- User preferences (voice model, speed)

### Session Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 SESSION MANAGER                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                      â”‚
â”‚  Session ID: "voice-uuid-123"                       â”‚
â”‚  Created: 2026-02-21T10:30:00Z                      â”‚
â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Conversation History (last 10 turns)       â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚  User: "What's the weather in Paris?"       â”‚   â”‚
â”‚  â”‚  OpenClaw: "It's 22Â°C and sunny in Paris."    â”‚   â”‚
â”‚  â”‚  User: "What about tomorrow?"                â”‚   â”‚
â”‚  â”‚  OpenClaw: "Tomorrow will be 19Â°C with..."   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  OpenClaw Context                           â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚  - Loaded file: project-roadmap.md          â”‚   â”‚
â”‚  â”‚  - Current directory: /workspace/project      â”‚   â”‚
â”‚  â”‚  - Tools used recently: web_search, file      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Audio Preferences                          â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚  - TTS Voice: "amy"                         â”‚   â”‚
â”‚  â”‚  - Speech Rate: 1.0x                        â”‚   â”‚
â”‚  â”‚  - Volume: 0.8                              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Implementation

```python
# voice_bridge/session_manager.py
from dataclasses import dataclass, field
from typing import List, Dict, Optional
from datetime import datetime, timedelta
import uuid

@dataclass
class VoiceSession:
    id: str = field(default_factory=lambda: f"voice-{uuid.uuid4().hex[:8]}")
    created_at: datetime = field(default_factory=datetime.utcnow)
    last_activity: datetime = field(default_factory=datetime.utcnow)
    
    # Conversation history
    history: List[Dict] = field(default_factory=list)
    max_history: int = 10
    
    # OpenClaw context references
    openclaw_session_id: Optional[str] = None
    loaded_files: List[str] = field(default_factory=list)
    working_directory: Optional[str] = None
    
    # Audio settings
    voice_model: str = "amy"
    speech_rate: float = 1.0
    
    def add_turn(self, user_text: str, assistant_text: str):
        """Add conversation turn"""
        self.history.append({
            "timestamp": datetime.utcnow().isoformat(),
            "user": user_text,
            "assistant": assistant_text
        })
        # Trim to max
        self.history = self.history[-self.max_history:]
        self.last_activity = datetime.utcnow()
    
    def to_openclaw_messages(self) -> List[Dict]:
        """Convert to OpenClaw message format"""
        messages = []
        for turn in self.history:
            messages.append({"role": "user", "content": turn["user"]})
            messages.append({"role": "assistant", "content": turn["assistant"]})
        return messages
    
    def is_expired(self, timeout_minutes: int = 30) -> bool:
        expiry = self.last_activity + timedelta(minutes=timeout_minutes)
        return datetime.utcnow() > expiry

class SessionManager:
    def __init__(self):
        self.sessions: Dict[str, VoiceSession] = {}
        self.openclaw_mapping: Dict[str, str] = {}  # voice_id â†’ openclaw_id
    
    def create_session(self) -> VoiceSession:
        session = VoiceSession()
        self.sessions[session.id] = session
        return session
    
    def get_session(self, session_id: str) -> Optional[VoiceSession]:
        session = self.sessions.get(session_id)
        if session and session.is_expired():
            del self.sessions[session_id]
            return None
        return session
    
    def link_openclaw_session(self, voice_id: str, openclaw_id: str):
        self.openclaw_mapping[voice_id] = openclaw_id
        session = self.sessions.get(voice_id)
        if session:
            session.openclaw_session_id = openclaw_id
    
    def cleanup_expired(self):
        expired = [
            sid for sid, s in self.sessions.items() 
            if s.is_expired()
        ]
        for sid in expired:
            del self.sessions[sid]
```

---

## Implementation Phases

### Phase 1: Foundation (Week 1-2)
**Goal:** Basic bidirectional flow

- [ ] Create new repo: `openclaw-voice-v2`
- [ ] Implement HTTP bridge to OpenClaw
- [ ] Add response filtering (heuristic fallback)
- [ ] Integrate existing STT (Faster-Whisper) + TTS (Piper)
- [ ] Basic session management
- [ ] Push-to-talk mode only

**Deliverable:** Voice input â†’ OpenClaw â†’ Voice output workflow

### Phase 2: Intelligence (Week 3-4)
**Goal:** Smart filtering + wake word

- [ ] OpenClaw plugin for metadata tagging
- [ ] Move from heuristic to metadata-based filtering
- [ ] Integrate wake word detection (Porcupine/OpenWakeWord)
- [ ] Hands-free activation
- [ ] Session persistence across turns

**Deliverable:** "Hey Hal, what's on my calendar?" â†’ spoken response

### Phase 3: Fluidity (Week 5-6)
**Goal:** Natural conversation flow

- [ ] WebSocket implementation for lower latency
- [ ] Interruption handling (barge-in)
- [ ] Streaming TTS (start speaking before full response)
- [ ] Conversation context (multi-turn memory)
- [ ] Error recovery and fallbacks

**Deliverable:** Fluid multi-turn conversations with interruptions

### Phase 4: Polish (Week 7-8)
**Goal:** Production-ready experience

- [ ] Voice activity detection (VAD) for natural endpointing
- [ ] Voice cloning for personalized assistant voice
- [ ] Multi-language support
- [ ] Configuration UI/web interface
- [ ] Performance optimization

**Deliverable:** Daily-driver voice interface

---

## GitHub Repository Structure

### V2 Repository: `openclaw-voice-v2`

```
openclaw-voice-v2/
â”œâ”€â”€ README.md                     # Project overview
â”œâ”€â”€ LICENSE                       # Apache 2.0
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ docs/                         # Documentation
â”‚   â”œâ”€â”€ architecture.md           # Detailed system design
â”‚   â”œâ”€â”€ api-reference.md            # OpenClaw integration API
â”‚   â”œâ”€â”€ configuration.md            # Setup guide
â”‚   â””â”€â”€ troubleshooting.md          # Common issues
â”‚
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ bridge/                   # Core bridge modules
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ openclaw_client.py     # HTTP/WebSocket client
â”‚   â”‚   â”œâ”€â”€ session_manager.py     # Session persistence
â”‚   â”‚   â””â”€â”€ response_filter.py     # Filtering logic
â”‚   â”‚
â”‚   â”œâ”€â”€ audio/                    # Audio pipeline
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ capture.py             # Microphone input
â”‚   â”‚   â”œâ”€â”€ stt_engine.py          # Whisper integration
â”‚   â”‚   â”œâ”€â”€ tts_engine.py          # Piper integration
â”‚   â”‚   â””â”€â”€ playback.py            # Audio output
â”‚   â”‚
â”‚   â”œâ”€â”€ wake/                     # Wake word detection
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ porcupine_wake.py      # Porcupine integration
â”‚   â”‚   â””â”€â”€ openwakeword_wake.py   # OpenWakeWord fallback
â”‚   â”‚
â”‚   â”œâ”€â”€ vad/                      # Voice Activity Detection
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ silero_vad.py          # Silero VAD
â”‚   â”‚
â”‚   â”œâ”€â”€ config/                   # Configuration
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ settings.py            # Pydantic settings
â”‚   â”‚   â””â”€â”€ default_config.yaml
â”‚   â”‚
â”‚   â””â”€â”€ main.py                   # Entry point
â”‚
â”œâ”€â”€ plugins/                      # OpenClaw plugins
â”‚   â””â”€â”€ response_filter_plugin.py  # Metadata tagging
â”‚
â”œâ”€â”€ tests/                        # Test suite
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ fixtures/
â”‚
â”œâ”€â”€ scripts/                      # Utility scripts
â”‚   â”œâ”€â”€ install.sh                # Installation script
â”‚   â”œâ”€â”€ setup-venv.sh             # Environment setup
â”‚   â””â”€â”€ download-models.sh        # Model downloader
â”‚
â”œâ”€â”€ docker/                       # Container support
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â””â”€â”€ examples/                     # Example configs & tests
    â”œâ”€â”€ openclaw-config.yaml
    â””â”€â”€ voice-config.yaml
```

### V1 Preserved

Keep `ray1caron/local-voice` intact as reference and fallback.

### Branching Strategy

```
main                    â† Stable releases (v2.0.x, v2.1.x)
â”œâ”€â”€ develop            â† Integration branch
â”‚   â”œâ”€â”€ feature/websocket
â”‚   â”œâ”€â”€ feature/wake-word
â”‚   â””â”€â”€ feature/interruptions
â”œâ”€â”€ release/v2.0.0
â””â”€â”€ hotfix/*
```

---

## GitHub Projects Setup

### Sprint Board: "Voice Interface v2 Development"

**Columns:**
1. ğŸ“‹ Backlog
2. ğŸ”„ Ready
3. ğŸƒ In Progress
4. ğŸ‘€ Code Review
5. âœ… Done

### Milestones

| Milestone | Target Date | Focus |
|-----------|-------------|-------|
| v2.0-alpha | Week 2 | Basic bidirectional flow |
| v2.0-beta | Week 4 | Wake word + filtering |
| v2.0.0 | Week 6 | WebSocket + interruptions |
| v2.1.0 | Week 8 | Polish + VAD |

### Label System

- `priority:critical` - Blocker
- `priority:high` - Important
- `priority:medium` - Nice to have
- `priority:low` - Future
- `type:bug` - Bug fix
- `type:feature` - New feature
- `component:stt` - Speech-to-text
- `component:tts` - Text-to-speech
- `component:bridge` - OpenClaw bridge
- `component:filter` - Response filtering
- `area:perf` - Performance
- `area:security` - Security

---

## Edge Cases & Mitigations

| Edge Case | Scenario | Mitigation |
|-----------|----------|------------|
| **Interruption** | User speaks while assistant is speaking | VAD detects new speech â†’ send `interruption` signal to OpenClaw â†’ cancel TTS queue â†’ process new input |
| **Long responses** | Tool chain produces 500+ tokens | Stream TTS incrementally; use sentence boundaries to chunk |
| **STT errors** | Whisper returns gibberish | Confidence threshold (>0.6); ask user to repeat |
| **Network failure** | OpenClaw unreachable | Queue locally; friendly voice error: "I'm having trouble connecting" |
| **Session timeout** | 30min silence | Graceful shutdown; wake word restarts new session |
| **Tool output filtering** | `exec` returns 1000 lines | Summarize before TTS: "I found 12 results, here are the top 3" |
| **Multi-turn context** | "What about yesterday?" | Session manager retains history; inject into OpenClaw context |
| **Wake word false positive** | TV says "Hey Hal" | Secondary confirmation or speaker identification |
| **Concurrent sessions** | Multiple users, one OpenClaw | Session isolation; user-specific voice profiles |

---

## Configuration Reference

### voice-config.yaml

```yaml
# Core settings
session:
  ttl_minutes: 30
  max_history: 10
  save_path: "~/.openclaw-voice/sessions"

# OpenClaw connection
openclaw:
  host: "localhost"
  port: 3000
  protocol: "websocket"  # or "http"
  api_version: "v1"
  
  # Integration settings
  response_filtering:
    mode: "metadata"     # or "heuristic"
    include_thinking: false
    include_tool_calls: false
  
  # Session linking
  session_mapping: true  # Link voice sessions to OpenClaw sessions

# Audio pipeline
audio:
  input:
    device: "default"
    sample_rate: 16000
    channels: 1
    chunk_size: 1024
  
  output:
    device: "default"
    volume: 0.8
    
  # Voice Activity Detection
  vad:
    enabled: true
    engine: "silero"     # or "webrtc"
    threshold: 0.5
    min_speech_duration_ms: 250
    min_silence_duration_ms: 500

# Speech-to-Text
stt:
  engine: "whisper"
  model: "medium"        # tiny, base, small, medium, large-v3
  device: "cuda"         # cuda, cpu
  compute_type: "float16"
  language: "en"
  
# Text-to-Speech
tts:
  engine: "piper"
  model: "en_US-amy-medium"
  speed: 1.0
  
  streaming:
    enabled: true
    sentence_chunking: true

# Wake Word
wake_word:
  enabled: true
  engine: "porcupine"    # or "openwakeword"
  
  porcupine:
    access_key: "${PICOVOICE_KEY}"  # env var
    keywords: ["computer", "jarvis"]
    sensitivity: 0.7
  
  openwakeword:
    model: "hey_hal"
    threshold: 0.5

# Conversation
conversation:
  barge_in:
    enabled: true          # Allow interruptions
    sensitivity: "medium"  # low, medium, high
  
  pause_timeout: 3.0     # Seconds of silence before turn end
  max_response_length: 500  # Tokens before summarization
```

---

## Success Metrics

| Metric | Target | Measurement |
|--------|--------|-------------|
| **Latency** | <2s end-to-end | Stopwatch: speech end â†’ audio start |
| **Accuracy** | >95% STT | Manual evaluation of 100 phrases |
| **Availability** | >99% uptime | Session crash rate |
| **User Satisfaction** | >4/5 | Informal rating after use |
| **Context Retention** | 5+ turns | Multi-turn accuracy test |

---

## OpenClaw Integration Checklist

To enable this voice interface, OpenClaw needs:

- [ ] `/api/voice/chat` endpoint with SSE streaming
- [ ] WebSocket `/voice` endpoint for bidirectional
- [ ] `voice_mode` parameter to enable filtering
- [ ] Response metadata tags (`type: final`, `type: thinking`, etc.)
- [ ] Session ID linking between voice and main sessions
- [ ] Interruption signal handling (cancel ongoing generation)

---

## Next Actions

1. **Create Repository**: `ray1caron/openclaw-voice-v2`
2. **Setup GitHub Project**: Sprint board with milestones
3. **OpenClaw RFC**: Propose API changes for response tagging
4. **Phase 1 Kickoff**: Implement HTTP bridge + fallback filtering
5. **Testing**: Daily dogfooding with Ray's OpenClaw instance

---

## Configuration Architecture

### Configuration Sources (Priority Order)

```
Environment Variables â†’ User Config (~/.config) â†’ Default Config (repo) â†’ Code Defaults
     (highest)            (user overrides)        (repository)         (fallbacks)
```

**Why this order:**
- **Env vars:** Secrets that shouldn't be in files (tokens, passwords)
- **User config ~/.config/voice-bridge-v2/:** Personal preferences (audio devices, voice settings)
- **Default config:** Sane defaults shipped with code
- **Code defaults:** Hardcoded fallbacks

### Key Configuration Decisions

| Decision | Rationale | Performance Impact |
|----------|-----------|-------------------|
| **Localhost OpenClaw** | Eliminates network latency | âœ… Critical for <2s target |
| **Minimal dependencies** | Reduce complexity | âœ… Manageable stack |
| **Config hot-reload** | Development convenience | âš ï¸ ~1-2% CPU (dev only) |
| **XDG compliance** | Standard locations | âœ… No impact |
| **Audio name discovery** | Resilient to changes | âœ… One-time cost |
| **Env vars for secrets** | Security | âœ… No impact |

### Configuration Files

**Repository defaults:** `config/default.yaml`
```yaml
bridge:
  openclaw:
    host: "localhost"
    port: 3000
    secure: false

audio:
  sample_rate: 16000
  channels: 1

stt:
  model: "medium"
  device: "cuda"
  
secret_from_env: "${OPENCLAW_API_KEY}"
```

**User overrides:** `~/.config/voice-bridge-v2/config.yaml`
```yaml
audio:
  input_device: "USB Audio"
  output_device: "Built-in Audio"

wake:
  keyword: "Hal"

logging:
  level: "DEBUG"
```

### Environment Variables

| Variable | Purpose | Example |
|----------|---------|---------|
| `VOICEBRIDGE_OPENCLAW_HOST` | OpenClaw server | `192.168.1.100` |
| `VOICEBRIDGE_OPENCLAW_API_KEY` | API authentication | `secret-token` |
| `GITHUB_TOKEN` | For issue creation | `ghp_...` |
| `VOICEBRIDGE_CONFIG_PATH` | Custom config location | `/etc/voice-bridge/` |
| `VOICEBRIDGE_LOG_LEVEL` | Runtime logging | `DEBUG` |

### Performance Targets

| Metric | Target | Measurement |
|--------|--------|-------------|
| **End-to-end latency** | <2s | Speech end â†’ audio start |
| **STT latency** | 300-500ms | Whisper medium on CUDA |
| **LLM TTFB** | <500ms | Time to first byte |
| **TTS latency** | 200ms | Piper generation |
| **Audio pipeline** | <50ms | Capture/playback |
| **Tool query** | 3-4s | With tool execution |

---

## User Feedback System

### Why Feedback is Critical

When humans talk to computers, they need **state awareness** to know:
- When to speak (is it listening?)
- When to wait (is it thinking?)
- If they were understood (did it hear me?)
- If something went wrong (error state)

Without feedback, users experience:
- **Uncertainty** â†’ "Did it hear me?"
- **Anxiety** â†’ "Should I repeat?"
- **Frustration** â†’ "Why isn't it responding?"
- **Over-talking** â†’ Interrupting before response

### States to Communicate

1. **Idle** â€” Waiting for wake word
2. **Listening** â€” Capturing speech (after wake word)
3. **Thinking** â€” Processing (STT â†’ OpenClaw â†’ TTS)
4. **Speaking** â€” Playing response
5. **Error** â€” Something went wrong

### Implementation Options

#### Option 1: Audio Feedback Only (Earcons)

**States:**
- **Wake detected:** Ascending chime ("ding-dong")
- **Listening start:** Single beep
- **Listening end:** Double beep
- **Thinking:** Subtle hum (optional)
- **Speaking:** No tone (TTS plays)
- **Error:** Descending tone ("uh-oh")

**Pros:**
- âœ… Works without screen (headless-friendly)
- âœ… User doesn't need to look at anything
- âœ… Natural for voice-only interaction
- âœ… Low latency (<5ms)

**Cons:**
- âŒ Can interfere with speech if not careful
- âŒ Harder to convey complex state

**Best for:** Headless setups, voice-only interaction

#### Option 2: Visual Feedback Only

**States:**
```
ğŸ”´ Idle (waiting for wake word)
ğŸŸ¢ Listening (after wake word, capturing speech)
ğŸ”µ Thinking (processing, STT â†’ OpenClaw â†’ TTS)
âšª Speaking (TTS playing)
ğŸŸ¡ Error (something went wrong)
```

**Implementation:**
- Desktop GUI window (tkinter/PyQt)
- System tray icon with color states
- LED control (if hardware available)
- Terminal status bar (for dev)

**Pros:**
- âœ… No audio interference with speech
- âœ… Clear visual hierarchy
- âœ… Can show detailed status

**Cons:**
- âŒ Requires screen
- âŒ User must look at screen

**Best for:** Desktop/laptop use with screen visible

#### Option 3: Combined Audio + Visual (Recommended)

**Implementation:**
- Visual indicator for primary state
- Audio earcons for state transitions only
- Detailed status in visual, simple cues in audio

**States:**
```
Visual: ğŸ”´ Idle    Audio: (none)
Visual: ğŸŸ¢ Listening    Audio: "ding" (wake detected)
Visual: ğŸ”µ Thinking    Audio: (none, or subtle hum)
Visual: âšª Speaking    Audio: (none, TTS plays)
Visual: ğŸŸ¡ Error    Audio: "uh-oh" tone
```

**Pros:**
- âœ… Best of both worlds
- âœ… Visual for detailed status
- âœ… Audio for immediate feedback
- âœ… Flexible (can disable audio if needed)

**Cons:**
- âŒ More complex to implement
- âŒ Requires both GUI and audio systems

**Best for:** General use, flexible deployment

### Performance Impact

| Metric | Without Feedback | With Feedback | Delta |
|--------|------------------|---------------|-------|
| CPU Usage | Baseline | +1-3% | Negligible |
| Memory | Baseline | +25MB | Acceptable |
| Latency | Baseline | +10-15ms | Negligible |
| User Experience | âŒ Uncertainty | âœ… Confidence | **Major improvement** |

### Best Practices Summary

1. **Immediate feedback** (<100ms) â€” Any user action gets instant response
2. **Progressive disclosure** â€” Simple state first, detail on demand
3. **Non-blocking** â€” Feedback doesn't prevent user action
4. **Consistent mapping** â€” Same state = same feedback every time
5. **Graceful degradation** â€” If visual fails, audio still works
6. **User control** â€” Can disable audio feedback if desired

### Recommended for Your Setup

**Phase 1 (MVP):** Audio + Terminal Status
- Audio earcons for state transitions
- Terminal status line for development
- Simple, no GUI dependencies

**Phase 2 (Enhancement):** Add Visual
- System tray icon
- Optional desktop widget
- LED support (if hardware added)

---

## Document References

- **FEEDBACK_DESIGN.md** â€” Full implementation guide with code examples
- **CONFIG_DISCUSSION.md** â€” Configuration architecture analysis
- **PROJECT.md** â€” Quick reference for development sessions
- **voice-assistant-plan-v2.md** â€” Main architecture document (this file)
